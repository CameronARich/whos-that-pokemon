{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who's That Pokémon? - Core Classifier\n",
    "\n",
    "This notebook focuses on the core classification task using the Kaggle Pokemon dataset. We'll implement and compare the following feature extraction techniques:\n",
    "\n",
    "1. HOG (Histogram of Oriented Gradients)\n",
    "2. SIFT (Scale-Invariant Feature Transform)\n",
    "3. PCA (Principal Component Analysis) on image features\n",
    "\n",
    "We'll train and test models using the internal Kaggle dataset first to establish a baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ML models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Feature extraction\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "# Display settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data\n",
    "\n",
    "First, let's load the metadata and explore the Kaggle dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "metadata_path = '../../data/kaggle_data/metadata.csv'\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Metadata shape: {metadata.shape}\")\n",
    "print(\"\\nFirst few rows of metadata:\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique Pokémon and distribution\n",
    "unique_pokemon = metadata['label'].unique()\n",
    "print(f\"Number of unique Pokémon: {len(unique_pokemon)}\")\n",
    "\n",
    "# Check image count distribution\n",
    "pokemon_counts = metadata['label'].value_counts()\n",
    "print(f\"\\nAverage images per Pokémon: {pokemon_counts.mean():.2f}\")\n",
    "print(f\"Min images for a Pokémon: {pokemon_counts.min()}\")\n",
    "print(f\"Max images for a Pokémon: {pokemon_counts.max()}\")\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pokemon_counts, bins=30)\n",
    "plt.title('Distribution of Images per Pokémon')\n",
    "plt.xlabel('Number of Images')\n",
    "plt.ylabel('Number of Pokémon Classes')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a few examples to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display sample Pokémon images\n",
    "def display_pokemon_examples(pokemon_name, num_examples=5):\n",
    "    # Get paths for this Pokémon\n",
    "    pokemon_data = metadata[metadata['label'] == pokemon_name]\n",
    "    sample_paths = pokemon_data['image_path'].sample(min(num_examples, len(pokemon_data))).values\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, axes = plt.subplots(1, len(sample_paths), figsize=(15, 3))\n",
    "    if len(sample_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Load and display each image\n",
    "    for i, path in enumerate(sample_paths):\n",
    "        img_path = os.path.join('../../data/kaggle_data/', path)\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"{pokemon_name} #{i+1}\")\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display examples of a few different Pokémon\n",
    "pokemon_examples = unique_pokemon[:3]  # First 3 Pokémon in the dataset\n",
    "for pokemon in pokemon_examples:\n",
    "    print(f\"\\nExamples of {pokemon}:\")\n",
    "    display_pokemon_examples(pokemon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction Functions\n",
    "\n",
    "Now let's implement our three feature extraction methods: HOG, SIFT, and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image_path, target_size=(128, 128), pixels_per_cell=(8, 8), \n",
    "                         cells_per_block=(2, 2), orientations=9):\n",
    "    \"\"\"Extract HOG features from an image\"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        # Resize to ensure consistent size\n",
    "        img = img.resize(target_size)\n",
    "        \n",
    "        # Convert to numpy array and grayscale\n",
    "        img_array = np.array(img)\n",
    "        gray_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Extract HOG features\n",
    "        features = hog(gray_img, orientations=orientations, \n",
    "                      pixels_per_cell=pixels_per_cell,\n",
    "                      cells_per_block=cells_per_block,\n",
    "                      block_norm='L2-Hys',\n",
    "                      visualize=False)\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features(image_path, target_size=(128, 128), n_features=128):\n",
    "    \"\"\"Extract SIFT features from an image\"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        # Resize to ensure consistent size\n",
    "        img = img.resize(target_size)\n",
    "        \n",
    "        # Convert to numpy array and grayscale\n",
    "        img_array = np.array(img)\n",
    "        gray_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Create SIFT detector\n",
    "        sift = cv2.SIFT_create()\n",
    "        \n",
    "        # Detect keypoints and compute descriptors\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray_img, None)\n",
    "        \n",
    "        # Handle cases with no or few keypoints\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            # Return zeros if no keypoints found\n",
    "            return np.zeros(n_features)\n",
    "        \n",
    "        # Create a fixed-length feature vector using bag-of-words approach\n",
    "        # For simplicity, we'll just average the descriptors and pad/truncate\n",
    "        avg_desc = np.mean(descriptors, axis=0)\n",
    "        if len(avg_desc) > n_features:\n",
    "            return avg_desc[:n_features]\n",
    "        elif len(avg_desc) < n_features:\n",
    "            return np.pad(avg_desc, (0, n_features - len(avg_desc)))\n",
    "        else:\n",
    "            return avg_desc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_apply_pca(images, feature_func, n_components=100, **kwargs):\n",
    "    \"\"\"Extract features from images and apply PCA\"\"\"\n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    features = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    for img_path in tqdm(images):\n",
    "        feat = feature_func(img_path, **kwargs)\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "            valid_paths.append(img_path)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Apply PCA\n",
    "    print(f\"Applying PCA to reduce dimensions to {n_components} components...\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_pca = pca.fit_transform(features)\n",
    "    \n",
    "    return features_pca, valid_paths, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Smaller Dataset for Initial Testing\n",
    "\n",
    "Since we have a large dataset with 1000 Pokémon classes, let's start by working with a smaller subset for initial testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller dataset with fewer Pokémon for initial testing\n",
    "n_pokemon_classes = 50  # Start with 50 Pokémon classes\n",
    "\n",
    "# Get the most common Pokémon (with most images)\n",
    "top_pokemon = pokemon_counts.index[:n_pokemon_classes]\n",
    "print(f\"Selected {len(top_pokemon)} Pokémon for initial testing\")\n",
    "\n",
    "# Filter the metadata to only include these Pokémon\n",
    "subset_metadata = metadata[metadata['label'].isin(top_pokemon)]\n",
    "print(f\"Subset size: {len(subset_metadata)} images\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "subset_metadata['label_encoded'] = label_encoder.fit_transform(subset_metadata['label'])\n",
    "\n",
    "# Look at the encoded labels\n",
    "encoder_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "print(f\"\\nFirst 5 class mappings: {list(encoder_mapping.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_paths = subset_metadata['image_path'].values\n",
    "y = subset_metadata['label_encoded'].values\n",
    "\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "    X_paths, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_paths)} images\")\n",
    "print(f\"Testing set: {len(X_test_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Features and Apply PCA\n",
    "\n",
    "Now let's extract features and apply PCA for dimension reduction. We'll do this for both HOG and SIFT features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full paths for images\n",
    "X_train_full_paths = [os.path.join('../../data/kaggle_data', path) for path in X_train_paths]\n",
    "X_test_full_paths = [os.path.join('../../data/kaggle_data', path) for path in X_test_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features and apply PCA\n",
    "print(\"Processing HOG features...\")\n",
    "hog_train_features, valid_train_paths_hog, pca_hog = extract_and_apply_pca(\n",
    "    X_train_full_paths, \n",
    "    extract_hog_features,\n",
    "    n_components=100,  # Reduce to 100 dimensions\n",
    "    target_size=(128, 128),\n",
    "    pixels_per_cell=(16, 16),  # Larger cells for fewer dimensions\n",
    "    orientations=9\n",
    ")\n",
    "\n",
    "# Get the corresponding labels for valid paths\n",
    "y_train_hog = [y_train[list(X_train_paths).index(os.path.relpath(path, '../../data/kaggle_data'))] \n",
    "              for path in valid_train_paths_hog]\n",
    "\n",
    "# Transform test features using the same PCA\n",
    "test_hog_features = []\n",
    "valid_test_paths_hog = []\n",
    "\n",
    "for img_path in tqdm(X_test_full_paths):\n",
    "    feat = extract_hog_features(\n",
    "        img_path, \n",
    "        target_size=(128, 128),\n",
    "        pixels_per_cell=(16, 16),\n",
    "        orientations=9\n",
    "    )\n",
    "    if feat is not None:\n",
    "        test_hog_features.append(feat)\n",
    "        valid_test_paths_hog.append(img_path)\n",
    "\n",
    "test_hog_features = np.array(test_hog_features)\n",
    "test_hog_features_pca = pca_hog.transform(test_hog_features)\n",
    "\n",
    "# Get the corresponding test labels\n",
    "y_test_hog = [y_test[list(X_test_paths).index(os.path.relpath(path, '../../data/kaggle_data'))] \n",
    "             for path in valid_test_paths_hog]\n",
    "\n",
    "print(f\"HOG features shape after PCA - Train: {hog_train_features.shape}, Test: {test_hog_features_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SIFT features and apply PCA\n",
    "print(\"Processing SIFT features...\")\n",
    "sift_train_features, valid_train_paths_sift, pca_sift = extract_and_apply_pca(\n",
    "    X_train_full_paths, \n",
    "    extract_sift_features,\n",
    "    n_components=100,\n",
    "    target_size=(128, 128),\n",
    "    n_features=128\n",
    ")\n",
    "\n",
    "# Get the corresponding labels for valid paths\n",
    "y_train_sift = [y_train[list(X_train_paths).index(os.path.relpath(path, '../../data/kaggle_data'))] \n",
    "               for path in valid_train_paths_sift]\n",
    "\n",
    "# Transform test features using the same PCA\n",
    "test_sift_features = []\n",
    "valid_test_paths_sift = []\n",
    "\n",
    "for img_path in tqdm(X_test_full_paths):\n",
    "    feat = extract_sift_features(\n",
    "        img_path, \n",
    "        target_size=(128, 128),\n",
    "        n_features=128\n",
    "    )\n",
    "    if feat is not None:\n",
    "        test_sift_features.append(feat)\n",
    "        valid_test_paths_sift.append(img_path)\n",
    "\n",
    "test_sift_features = np.array(test_sift_features)\n",
    "test_sift_features_pca = pca_sift.transform(test_sift_features)\n",
    "\n",
    "# Get the corresponding test labels\n",
    "y_test_sift = [y_test[list(X_test_paths).index(os.path.relpath(path, '../../data/kaggle_data'))] \n",
    "              for path in valid_test_paths_sift]\n",
    "\n",
    "print(f\"SIFT features shape after PCA - Train: {sift_train_features.shape}, Test: {test_sift_features_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate Models\n",
    "\n",
    "Now let's train models on our extracted features and evaluate their performance. We'll try both SVM and Random Forest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test, model_name=\"SVM\", feature_type=\"HOG\"):\n",
    "    \"\"\"Train a model and evaluate its performance\"\"\"\n",
    "    print(f\"\\n{model_name} with {feature_type} features:\")\n",
    "    \n",
    "    # Select model\n",
    "    if model_name == \"SVM\":\n",
    "        model = SVC(probability=True, random_state=42)\n",
    "    elif model_name == \"RandomForest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsClassifier(n_neighbors=5)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[encoder_mapping[i] for i in np.unique(y_test)], zero_division=0))\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models with HOG features\n",
    "models_hog = {}\n",
    "accuracies_hog = {}\n",
    "\n",
    "for model_name in [\"SVM\", \"RandomForest\", \"KNN\"]:\n",
    "    models_hog[model_name], accuracies_hog[model_name] = train_and_evaluate(\n",
    "        hog_train_features, y_train_hog,\n",
    "        test_hog_features_pca, y_test_hog,\n",
    "        model_name=model_name,\n",
    "        feature_type=\"HOG\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models with SIFT features\n",
    "models_sift = {}\n",
    "accuracies_sift = {}\n",
    "\n",
    "for model_name in [\"SVM\", \"RandomForest\", \"KNN\"]:\n",
    "    models_sift[model_name], accuracies_sift[model_name] = train_and_evaluate(\n",
    "        sift_train_features, y_train_sift,\n",
    "        test_sift_features_pca, y_test_sift,\n",
    "        model_name=model_name,\n",
    "        feature_type=\"SIFT\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results and Compare Methods\n",
    "\n",
    "Let's visualize and compare the performance of different feature extraction methods and classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare accuracy data for plotting\n",
    "model_names = list(models_hog.keys())\n",
    "hog_acc = [accuracies_hog[model] for model in model_names]\n",
    "sift_acc = [accuracies_sift[model] for model in model_names]\n",
    "\n",
    "# Set up the bar plot\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(model_names))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(index, hog_acc, bar_width, label='HOG + PCA')\n",
    "bars2 = ax.bar(index + bar_width, sift_acc, bar_width, label='SIFT + PCA')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Classifier')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Performance by Feature Type')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(model_names)\n",
    "ax.legend()\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Confusion and Misclassifications\n",
    "\n",
    "Let's visualize some of the confusion and examine misclassifications for our best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model overall\n",
    "all_accuracies = {}\n",
    "for model_name in model_names:\n",
    "    all_accuracies[f\"HOG_{model_name}\"] = accuracies_hog[model_name]\n",
    "    all_accuracies[f\"SIFT_{model_name}\"] = accuracies_sift[model_name]\n",
    "\n",
    "best_model_name = max(all_accuracies, key=all_accuracies.get)\n",
    "best_acc = all_accuracies[best_model_name]\n",
    "print(f\"Best model: {best_model_name} with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# Get predictions from the best model\n",
    "feature_type, model_type = best_model_name.split('_')\n",
    "if feature_type == \"HOG\":\n",
    "    best_X_test = test_hog_features_pca\n",
    "    best_y_test = y_test_hog\n",
    "    best_model = models_hog[model_type]\n",
    "    best_valid_paths = valid_test_paths_hog\n",
    "else: # SIFT\n",
    "    best_X_test = test_sift_features_pca\n",
    "    best_y_test = y_test_sift\n",
    "    best_model = models_sift[model_type]\n",
    "    best_valid_paths = valid_test_paths_sift\n",
    "\n",
    "# Get predictions\n",
    "best_preds = best_model.predict(best_X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(best_y_test, best_preds)\n",
    "\n",
    "# Plot confusion matrix as heatmap (top 10 classes only for visibility)\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_classes = 10\n",
    "class_names = [encoder_mapping[i] for i in range(top_classes)]\n",
    "conf_subset = conf_matrix[:top_classes, :top_classes]\n",
    "\n",
    "plt.imshow(conf_subset, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.title(f'Confusion Matrix for Top {top_classes} Classes\\n({best_model_name})')\n",
    "plt.xticks(range(top_classes), class_names, rotation=90)\n",
    "plt.yticks(range(top_classes), class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(top_classes):\n",
    "    for j in range(top_classes):\n",
    "        plt.text(j, i, conf_subset[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if conf_subset[i, j] > conf_subset.max()/2 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize misclassifications\n",
    "def display_misclassifications(y_true, y_pred, image_paths, n_examples=5):\n",
    "    \"\"\"Display examples of misclassified Pokémon\"\"\"\n",
    "    # Find misclassifications\n",
    "    misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "    \n",
    "    # Select a random subset\n",
    "    if len(misclassified_indices) > n_examples:\n",
    "        selected_indices = np.random.choice(misclassified_indices, n_examples, replace=False)\n",
    "    else:\n",
    "        selected_indices = misclassified_indices\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, axes = plt.subplots(1, len(selected_indices), figsize=(15, 3))\n",
    "    if len(selected_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Display each misclassified example\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        img_path = image_paths[idx]\n",
    "        true_label = encoder_mapping[y_true[idx]]\n",
    "        pred_label = encoder_mapping[y_pred[idx]]\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display some misclassified examples\n",
    "print(f\"Misclassification examples from the best model ({best_model_name}):\")\n",
    "display_misclassifications(best_y_test, best_preds, best_valid_paths, n_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "We've implemented and compared HOG and SIFT feature extraction methods, both with PCA dimension reduction, and trained several classifiers on the Kaggle Pokémon dataset. Here are our findings and potential next steps:\n",
    "\n",
    "### Key Findings:\n",
    "1. (Results will be filled in after running)\n",
    "2. (Results will be filled in after running)\n",
    "3. (Results will be filled in after running)\n",
    "\n",
    "### Next Steps:\n",
    "1. Scale the approach to include more Pokémon classes (beyond the 50 we started with)\n",
    "2. Try combining features from different extraction methods\n",
    "3. Analyze performance by Pokémon generation or type\n",
    "4. Compare with generation-specific sprite data\n",
    "5. Investigate the most consistently misclassified Pokémon to understand limitations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}